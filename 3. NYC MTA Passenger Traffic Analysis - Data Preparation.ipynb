{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_logfiles():\n",
    "    url = 'http://web.mta.info/developers/'\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from bs4 import SoupStrainer\n",
    "    r = requests.get(url+'turnstile.html')\n",
    "    soup = BeautifulSoup(r.text, parse_only=SoupStrainer('a', href=True))\n",
    "    log_links = [ url + link['href'] for link in soup.find_all('a')]\n",
    "    log_links = [log_link for log_link in log_links if log_link.find('.txt') >=0 or log_link.find('.xls') >= 0]\n",
    "    for log_link in log_links:\n",
    "        download_file(log_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url):\n",
    "    import urllib\n",
    "    filename = url.split('/')[-1]\n",
    "    urllib.urlretrieve (url, filename)\n",
    "    print 'Successful downloaded ', filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_logfiles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def download_file(url):\n",
    "    import urllib\n",
    "    filename = url.split('/')[-1]\n",
    "    urllib.urlretrieve (url, filename)\n",
    "    print 'Successful downloaded ', filename\n",
    "\n",
    "def download_logfiles():\n",
    "    url = 'http://web.mta.info/developers/'\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from bs4 import SoupStrainer\n",
    "    r = requests.get(url+'turnstile.html')\n",
    "    soup = BeautifulSoup(r.text, parse_only=SoupStrainer('a', href=True))\n",
    "    log_links = [ url + link['href'] for link in soup.find_all('a')]\n",
    "    log_links = [log_link for log_link in log_links if log_link.find('.txt') >=0 or log_link.find('.xls') >= 0]\n",
    "    for log_link in log_links:\n",
    "        download_file(log_link)\n",
    "# download_logfiles()\n",
    "\n",
    "def split_log_lines(input_filename, output_filename):\n",
    "    fp = open(input_filename, 'r')\n",
    "    fp_tar = open(output_filename, 'a')\n",
    "    for line in fp:\n",
    "        dat = line.strip().split(',')\n",
    "        for i in range(3,len(dat),5):\n",
    "            fp_tar.write(','.join(dat[0:3]+dat[i:i+2]+dat[i+3:i+5]) + '\\n')\n",
    "    fp.close()\n",
    "    fp_tar.close()\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def split_log_lines(input_filename, raw_data_list):\n",
    "    fp = open(input_filename, 'r')\n",
    "    for line in fp:\n",
    "        dat = line.strip().split(',')\n",
    "        for i in range(3, len(dat), 5):\n",
    "            date_temp = dat[i].split('-')\n",
    "            dat[i] = '20' + '-'.join(date_temp[2: 3] + date_temp[0: 2])\n",
    "            raw_data_list.append(','.join(dat[0:3] + dat[i:i + 2] + dat[i + 3:i + 5]))\n",
    "    fp.close()\n",
    "raw_data_list = []\n",
    "logs_y13 = [filename for filename in os.listdir(\n",
    "    './') if filename.find('turnstile_13') >= 0 and filename.find('.txt') >= 0]\n",
    "for filename in logs_y13:\n",
    "    split_log_lines(filename, raw_data_list)\n",
    "split_log_lines('turnstile_140104.txt', raw_data_list)\n",
    "raw_data_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bisect\n",
    "import datetime\n",
    "station_name = {}\n",
    "fp = open(\"Remote-Booth-Station.csv\", 'r')\n",
    "for line in fp:\n",
    "    line = line.strip().split(',')\n",
    "    station_name['%s_%s' % (line[1], line[0])] = line[2]\n",
    "fp.close()\n",
    "\n",
    "\n",
    "class turnstile:\n",
    "    days_per_year = 365\n",
    "    blocks_per_day = 6\n",
    "    nan = False\n",
    "    __overall_nanPercentage__ = 0.5  # less than 50% valid log, this turnstile will be ignored in further calculation\n",
    "    __oneblock_nanPercentage__ = 0.2  # less than 20% valid log in a time block, the NaN days will be given value 0\n",
    "    __max_possible__ = 50000  # max possible passengers to exit/enter this turnstile\n",
    "    __max_downtime__ = 86400  # missing value longer then two days, won't do interpolations\n",
    "    __data_exception_times__ = 10.0  # if one value is 10 times larger than its average value, abandon this data\n",
    "    def __init__(self, line_list, year):\n",
    "        __temp_line1__ = line_list[0].split(',')\n",
    "        self.year = year\n",
    "        self.days_per_year = (datetime.datetime(year+1, 1, 1)-datetime.datetime(year,1,1)).days\n",
    "        self.CA = __temp_line1__[0]\n",
    "        self.UNIT = __temp_line1__[1]\n",
    "        self.SCP = __temp_line1__[2]\n",
    "        self.ENTRY = self.get_raw_entry(line_list)\n",
    "        self.EXIT = self.get_raw_exit(line_list)\n",
    "        self.entry_sum = np.sum(self.ENTRY)\n",
    "        self.exit_sum = np.sum(self.EXIT)\n",
    "        self.busyness = self.entry_sum + self.exit_sum\n",
    "        self.station_name = self.get_station_name()\n",
    "\n",
    "    def get_station_name(self):\n",
    "        station_code = self.CA + '_' + self.UNIT\n",
    "        if station_code in station_name:\n",
    "            return station_name[station_code]\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    def get_cumu_list(self, raw_list):\n",
    "        list_cum = np.zeros(self.days_per_year * self.blocks_per_day + 1, dtype=float)\n",
    "        list_cum.fill(np.NaN)\n",
    "        init_time = '%d-01-01 00:00:00' % self.year\n",
    "        end_time = '%d-01-01 00:00:00' % (self.year + 1)\n",
    "        temp_time = datetime.datetime.strptime(init_time, '%Y-%m-%d %H:%M:%S')\n",
    "        count = 0\n",
    "        while str(temp_time) <= end_time:\n",
    "            neigh = bisect.bisect(raw_list, str(temp_time))\n",
    "            inter_val = self.GetInterpT_V(raw_list[neigh - 1: neigh + 1], temp_time)\n",
    "            list_cum[count] = inter_val\n",
    "            count += 1\n",
    "            temp_time += datetime.timedelta(hours=4)\n",
    "        return self.get_delta_list(list_cum)\n",
    "\n",
    "    def GetInterpT_V(self, timeab, temp_time):\n",
    "        # timeab = [\"time1val1\", \"time2val2\"]\n",
    "        if len(timeab) < 2:\n",
    "            return np.NaN\n",
    "        time2, value2 = timeab[1].split(',')\n",
    "        if str(temp_time) == time2:\n",
    "            return int(value2)\n",
    "        time1, value1 = timeab[0].split(',')\n",
    "        if str(temp_time) == time1:\n",
    "            return int(value1)\n",
    "        if value1 == value2:\n",
    "            return int(value1)\n",
    "        value1, value2 = int(value1), int(value2)\n",
    "        if value2 < value1:\n",
    "            return np.NaN\n",
    "        time1 = datetime.datetime.strptime(time1, '%Y-%m-%d %H:%M:%S')\n",
    "        time2 = datetime.datetime.strptime(time2, '%Y-%m-%d %H:%M:%S')\n",
    "        time1 = (temp_time - time1).total_seconds()\n",
    "        time2 = (time2 - temp_time).total_seconds()\n",
    "        if time1 > self.__max_downtime__ or time2 > self.__max_downtime__:\n",
    "            return np.NaN\n",
    "        inter_value = value1 + (value2-value1) * float(time1) / (time1+time2)\n",
    "        # two-point linear interpolation\n",
    "        return int(inter_value)\n",
    "\n",
    "    def get_delta_list(self, cum_list):\n",
    "        delta_list = np.ndarray((self.days_per_year, self.blocks_per_day), dtype=float)\n",
    "        delta_list.fill(np.NaN)\n",
    "        nan_count = 0\n",
    "        for i in xrange(self.days_per_year):\n",
    "            for j in xrange(self.blocks_per_day):\n",
    "                index_in_cum = i*self.blocks_per_day+j\n",
    "                if np.isnan(cum_list[index_in_cum]) or np.isnan(cum_list[index_in_cum+1]):\n",
    "                    delta_list[i, j] = np.NaN\n",
    "                    nan_count += 1\n",
    "                else:\n",
    "                    delta = cum_list[index_in_cum+1] - cum_list[index_in_cum]\n",
    "                    if delta < 0 or delta > self.__max_possible__:\n",
    "                        delta_list[i, j] = np.NaN\n",
    "                        nan_count += 1\n",
    "                    else:\n",
    "                        delta_list[i, j] = delta\n",
    "        self.down_blocks = nan_count\n",
    "        if float(nan_count) / (self.days_per_year * self.blocks_per_day) > self.__overall_nanPercentage__:\n",
    "            self.nan = True\n",
    "        for j in xrange(self.blocks_per_day):\n",
    "            valid_values = [x for x in delta_list[:, j] if not np.isnan(x)]\n",
    "            if len(valid_values) == 0:\n",
    "                    delta_list[:, j] = 0\n",
    "                    continue\n",
    "            col_mean = np.mean(valid_values)\n",
    "            for i in xrange(self.days_per_year):\n",
    "                if np.isnan(delta_list[i, j]):\n",
    "                    if float(len(valid_values)) / self.days_per_year < self.__oneblock_nanPercentage__:\n",
    "                        delta_list[i, j] = 0\n",
    "                    else:\n",
    "                        delta_list[i, j] = col_mean\n",
    "                if delta_list[i, j] > col_mean * self.__data_exception_times__:\n",
    "                    valid_len = len(valid_values) - 1\n",
    "                    delta_list[i, j] = (col_mean*(valid_len+1) - delta_list[i, j])/valid_len\n",
    "        delta_list_int = np.ndarray((self.days_per_year, self.blocks_per_day), dtype=int)\n",
    "        for i in xrange(self.days_per_year):\n",
    "            for j in xrange(self.blocks_per_day):\n",
    "                delta_list_int[i][j] = int(delta_list[i][j])\n",
    "        return delta_list_int\n",
    "\n",
    "    def get_raw_entry(self, line_list):\n",
    "        raw_entry_list = []\n",
    "        for logs in line_list:\n",
    "            log = logs.split(',')\n",
    "            raw_entry_list.append('%s %s,%s' % (log[3], log[4], log[5]))\n",
    "        return self.get_cumu_list(raw_entry_list)\n",
    "\n",
    "    def get_raw_exit(self, line_list):\n",
    "        raw_exit_list = []\n",
    "        for logs in line_list:\n",
    "            log = logs.split(',')\n",
    "            raw_exit_list.append('%s %s,%s' % (log[3], log[4], log[6]))\n",
    "        return self.get_cumu_list(raw_exit_list)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '%s_%s_%s %d %s' % (self.CA, self.UNIT, self.SCP, self.business, self.station_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns = {}\n",
    "curr_turn = raw_data_list[0].split(',')\n",
    "turn_start_index = 0\n",
    "for i in xrange(len(raw_data_list)):\n",
    "    line = raw_data_list[i].split(',')\n",
    "    if line[0] != curr_turn[0] or line[1] != curr_turn[1] or line[2] != curr_turn[2]:\n",
    "        turns['_'.join(curr_turn[0:3])] = turnstile(raw_data_list[turn_start_index: i], 2013)\n",
    "        turn_start_index = i\n",
    "        #if line[0] != curr_turn[0]:\n",
    "        #    print '{percent:.2%}'.format(percent=float(i)/len(raw_data_list)) + ' logs finished'\n",
    "        curr_turn = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4618\n"
     ]
    }
   ],
   "source": [
    "print len(turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary 'turns' includes the turnstile objects for all the recorded turntiles in 2013. The turnstile(.) is the constructor for a turnstile object. Each turnstile object contains the cleaned exits/entries 4 hour counts. The EXIT/ENTRY are the 365*6 2D array, for the 0am, 4am, 8am, 12pm, 16pm, 20pm points of each day. (e.g. a value in 0am gives the counts from 0am to 4am.) Before finding the counts in each time interval, linear interpolation is applied to impute the missing data of the data stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.What is the total number of entries & exits across the subway system for August 1, 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5595593\n",
      "4437761\n"
     ]
    }
   ],
   "source": [
    "days0801 = (datetime.datetime.strptime(\"2013-08-01 12:00:00\", '%Y-%m-%d %H:%M:%S') - datetime.datetime.strptime(\"2013-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')).days\n",
    "print sum([sum(turns[i].ENTRY[days0801]) for i in turns])\n",
    "print sum([sum(turns[i].EXIT[days0801]) for i in turns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of entries across the subway system for August 1, 2013 is 5595593, whereas the total number of exits across the subway system for August 1, 2013 is 4437761."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Letâ€™s define the busy-ness as sum of entry & exit count. What station was the busiest on August 1, 2013? What turnstile was the busiest on that date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10493, 'W 4 ST-WASH SQ', 'N083_R138_01-00-00'),\n",
       " (10566, '42 ST-GRD CNTRL', 'R238_R046_00-00-01'),\n",
       " (11078, '42 ST-GRD CNTRL', 'R240_R047_00-00-00'),\n",
       " (11523, '86 ST', 'R249_R179_01-00-09'),\n",
       " (11845, '42 ST-PA BUS TE', 'N063A_R011_00-00-00')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [(sum(turns[i].ENTRY[days0801])+sum(turns[i].EXIT[days0801]), turns[i].station_name, i) for i in turns]\n",
    "a.sort()\n",
    "a[-5:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The busiest turnstile on August 1, 2013 is '42 ST-PA BUS TE', 'N063A_R011_00-00-00'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86 ST', 64744051),\n",
       " ('14 ST-UNION SQ', 68162945),\n",
       " ('34 ST-HERALD SQ', 73900262),\n",
       " ('42 ST-GRD CNTRL', 87805999),\n",
       " ('34 ST-PENN STA', 101597301)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "b = {}\n",
    "for x in turns:\n",
    "    if turns[x].station_name in b:\n",
    "        b[turns[x].station_name] += turns[x].busyness\n",
    "    else:\n",
    "        b[turns[x].station_name] = turns[x].busyness\n",
    "sorted(b.items(), key=operator.itemgetter(1))[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The busiest station on August 1, 2013 is '34 ST-PENN STA'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What were the busiest and least-busy stations in the system over all of July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('14 ST-UNION SQ', 5555751),\n",
       " ('42 ST-TIMES SQ', 5591576),\n",
       " ('34 ST-HERALD SQ', 6198209),\n",
       " ('42 ST-GRD CNTRL', 7456885),\n",
       " ('34 ST-PENN STA', 8596114)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days0701 = (datetime.datetime.strptime(\"2013-07-01 12:00:00\", '%Y-%m-%d %H:%M:%S') - datetime.datetime.strptime(\"2013-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')).days\n",
    "days0801 = (datetime.datetime.strptime(\"2013-08-01 12:00:00\", '%Y-%m-%d %H:%M:%S') - datetime.datetime.strptime(\"2013-01-01 00:00:00\", '%Y-%m-%d %H:%M:%S')).days\n",
    "import operator\n",
    "c = {}\n",
    "for x in turns:\n",
    "    summation = np.sum(turns[x].ENTRY[days0701:days0801]) + np.sum(turns[x].EXIT[days0701:days0801])\n",
    "    if turns[x].station_name in c:\n",
    "        c[turns[x].station_name] += summation\n",
    "    else:\n",
    "        c[turns[x].station_name] = summation\n",
    "SortResult=sorted(c.items(), key=operator.itemgetter(1))\n",
    "SortResult[-5:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the busiest stations in the system over all of July 2013 the corresponding entry/exit total counts. Below are the five least-busy stations in the system over all of July 2013 the corresponding entry/exit total counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('LGA AIRPORT CTB', 0),\n",
       " ('AQUEDUCT TRACK', 265),\n",
       " ('BROAD CHANNEL', 8391),\n",
       " ('ORCHARD BEACH', 16763),\n",
       " ('TOMPKINSVILLE', 18357)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SortResult[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the first 10 'C/A'-'UNIT' pairs (appeared in the raw_data_list) that are not found in the Remote-Booth-Station.csv.  Remove '[:10]' part for the whole list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R028_A082',\n",
       " 'R028_A082',\n",
       " 'R202_N330',\n",
       " 'R202_N330',\n",
       " 'R168_R169',\n",
       " 'R202_N330',\n",
       " 'R202_N330',\n",
       " 'R202_N330',\n",
       " 'R001_R101',\n",
       " 'R001_R101']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[turns[x].UNIT+'_'+turns[x].CA for x in turns if turns[x].station_name=='Unknown'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Which station had the highest average number of entries between midnight & 4am on Fridays in July 2013?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 192, 199, 206]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fridays = [i for i in range(1,32) if datetime.datetime(2013,7,i).isoweekday()==5]\n",
    "fridays = [(datetime.datetime(2013,7,i)-datetime.datetime(2013,1,1)).days for i in fridays]\n",
    "fridays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('W 4 ST-WASH SQ', 13250),\n",
       " ('34 ST-PENN STA', 19356),\n",
       " ('42 ST-PA BUS TE', 21553),\n",
       " ('14 ST-UNION SQ', 23022),\n",
       " ('42 ST-TIMES SQ', 23885)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "d = {}\n",
    "for x in turns:\n",
    "    summation = np.sum([np.sum(turns[x].ENTRY[i,0]) for i in fridays])\n",
    "    if turns[x].station_name in d:\n",
    "        d[turns[x].station_name] += summation\n",
    "    else:\n",
    "        d[turns[x].station_name] = summation\n",
    "sorted(d.items(), key=operator.itemgetter(1))[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The '42 ST-TIMES SQ' station had the highest average number of entries between midnight & 4am on Fridays in July 2013, which is 23885."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What stations have seen the most usage growth/decline in the last year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_log_lines2(input_filename, raw_data_list):\n",
    "    fp = open(input_filename, 'r')\n",
    "    for line in fp:\n",
    "        dat = line.strip().split(',')\n",
    "        if len(dat)==11:\n",
    "            date_temp = dat[6].split('/')\n",
    "            dat[6] = '-'.join(date_temp[2:] + date_temp[0: 2])\n",
    "            if dat[10] != 'EXITS':\n",
    "                raw_data_list.append(','.join(dat[0:3] + dat[6:8] + dat[9:11]))\n",
    "        else:\n",
    "            for i in range(3, len(dat), 5):\n",
    "                date_temp = dat[i].split('-')\n",
    "                dat[i] = '20' + '-'.join(date_temp[2: 3] + date_temp[0: 2])\n",
    "                raw_data_list.append(','.join(dat[0:3] + dat[i:i + 2] + dat[i + 3:i + 5]))\n",
    "    fp.close()\n",
    "def raw_data_list_f(file_head,addi_file_name):\n",
    "    raw_data_list = []\n",
    "    logs_y = [filename for filename in os.listdir(\n",
    "        './') if filename.find(file_head) >= 0 and filename.find('.txt') >= 0]\n",
    "    for filename in logs_y:\n",
    "        split_log_lines2(filename, raw_data_list)\n",
    "    split_log_lines2(addi_file_name, raw_data_list)\n",
    "    return raw_data_list\n",
    "def turns_f(raw_data_list,year):\n",
    "    turns = {}\n",
    "    curr_turn = raw_data_list[0].split(',')\n",
    "    turn_start_index = 0\n",
    "    for i in xrange(len(raw_data_list)):\n",
    "        line = raw_data_list[i].split(',')\n",
    "        if line[0] != curr_turn[0] or line[1] != curr_turn[1] or line[2] != curr_turn[2]:\n",
    "            turns['_'.join(curr_turn[0:3])] = turnstile(raw_data_list[turn_start_index: i], year)\n",
    "            turn_start_index = i\n",
    "            #if line[0] != curr_turn[0]:\n",
    "            #    print '{percent:.2%}'.format(percent=float(i)/len(raw_data_list)) + ' logs finished'\n",
    "            curr_turn = line\n",
    "    return turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_list14=raw_data_list_f('turnstile_14','turnstile_150103.txt')\n",
    "raw_data_list14.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a bad record in the raw data files and here we fix it by popping the bad data out of the raw_data_list14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N329A,R201,01-06-00,20REGUL\n",
      "4701943\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in raw_data_list14:\n",
    "    if len(i.split(','))<7:\n",
    "        print i\n",
    "        print count\n",
    "        break\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N329A,R201,01-06-00,20REGUL',\n",
       " 'N329A,R201,01-06-00,20WOODHAVEN BLVD,MR,12/02/2014,16:00:00',\n",
       " 'N329A,R201,01-06-01,2013-12-28,00:00:00,005611151,002672493',\n",
       " 'N329A,R201,01-06-01,2013-12-28,04:00:00,005611157,002672538']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_list14[4701943:4701947]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N329A,R201,01-06-00,20WOODHAVEN BLVD,MR,12/02/2014,16:00:00'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_list14.pop(4701943)\n",
    "raw_data_list14.pop(4701943)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns14=turns_f(raw_data_list14,2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data_list15=raw_data_list_f('turnstile_15','turnstile_160102.txt')\n",
    "raw_data_list15.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "turns15=turns_f(raw_data_list15,2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def turns_year_count(turns):\n",
    "    c = {}\n",
    "    for x in turns:\n",
    "        summation = turns[x].busyness\n",
    "        if turns[x].station_name in c:\n",
    "            c[turns[x].station_name] += summation\n",
    "        else:\n",
    "            c[turns[x].station_name] = summation\n",
    "    return pd.DataFrame(c.items(),columns=('Station','AnnualCounts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dic14=turns_year_count(turns14)\n",
    "Dic15=turns_year_count(turns15)\n",
    "Dic1415=pd.merge(Dic14,Dic15,left_on=['Station'], right_on=['Station'], how='inner')\n",
    "Dic1415['OffSet']=Dic1415['AnnualCounts_y']-Dic1415['AnnualCounts_x']\n",
    "Dic1415.columns=('Station','AnnualCounts2014','AnnualCounts2015','OffSet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Station that have seen the most usage growth in the last year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>AnnualCounts2014</th>\n",
       "      <th>AnnualCounts2015</th>\n",
       "      <th>OffSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>WHITEHALL ST</td>\n",
       "      <td>4682873</td>\n",
       "      <td>9377889</td>\n",
       "      <td>4695016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Station  AnnualCounts2014  AnnualCounts2015   OffSet\n",
       "214  WHITEHALL ST           4682873           9377889  4695016"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic1415[Dic1415['OffSet']==max(Dic1415['OffSet'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Station that have seen the most usage decline in the last year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>AnnualCounts2014</th>\n",
       "      <th>AnnualCounts2015</th>\n",
       "      <th>OffSet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57 ST-7 AVE</td>\n",
       "      <td>15030482</td>\n",
       "      <td>10775813</td>\n",
       "      <td>-4254669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Station  AnnualCounts2014  AnnualCounts2015   OffSet\n",
       "5  57 ST-7 AVE          15030482          10775813 -4254669"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dic1415[Dic1415['OffSet']==min(Dic1415['OffSet'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 What dates are the least busy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DayTotal=[0]*365\n",
    "for x in turns15:\n",
    "    a=[np.sum(turns15[x].ENTRY[i,:]) for i in range(365)]\n",
    "    day_count = range(1,366)\n",
    "    DayTotal=map(lambda x,y:x+y,DayTotal,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 1365488), (359, 1991703), (18, 2203244), (46, 2393195), (330, 2400279)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DayTotal = sorted(zip(range(1,365),DayTotal), key=lambda x:x[1])\n",
    "DayTotal[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-01-27\n"
     ]
    }
   ],
   "source": [
    "print (datetime.datetime(2015,1,1)+datetime.timedelta(days=DayTotal[0][0]-1)).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2015-01-27 is the least busy. The other four dates are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-12-25\n",
      "2015-01-18\n",
      "2015-02-15\n",
      "2015-11-26\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5,1):\n",
    "    print (datetime.datetime(2015,1,1)+datetime.timedelta(days=DayTotal[i][0]-1)).date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Could you identify days on which stations were not operating at full capacity or closed entirely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I build a class for the Station objects. Note that I defined a station a low capacity one if only 50% of its tenstiles are in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Station:\n",
    "    __low_capacity_limit__ = 0.5\n",
    "    \n",
    "    def __init__(self, station_name):\n",
    "        self.name = station_name\n",
    "        self.turn_num_year = {}\n",
    "        self.turn_name_year = {}\n",
    "        self.turn_day_year = {}\n",
    "        self.low_capacity_days = {}\n",
    "        self.closed_days = {}\n",
    "        self.turnstiles = []\n",
    "\n",
    "    def add_turn(self, turn):\n",
    "        curr_turn_name = '%s_%s_%s' % (turn.CA, turn.UNIT, turn.SCP)\n",
    "        if turn.year in self.turn_num_year:\n",
    "            self.turn_num_year[turn.year] += 1\n",
    "        else:\n",
    "            self.turn_num_year[turn.year] = 1\n",
    "        if turn.year in self.turn_name_year:\n",
    "            self.turn_name_year[turn.year].append(curr_turn_name)\n",
    "        else:\n",
    "            self.turn_name_year[turn.year] = [curr_turn_name]\n",
    "        if curr_turn_name not in self.turnstiles:\n",
    "            self.turnstiles.append(curr_turn_name)\n",
    "        if turn.year not in self.turn_day_year:\n",
    "            self.turn_day_year[turn.year] = [0]*turn.days_per_year\n",
    "        for i in xrange(turn.days_per_year):\n",
    "            if np.sum(turn.EXIT[i]) + np.sum(turn.ENTRY[i]) > 0:\n",
    "                self.turn_day_year[turn.year][i] += 1\n",
    "    \n",
    "    def get_closed_days(self, year):\n",
    "        closed_days = []\n",
    "        if year not in self.turn_day_year:\n",
    "            self.closed_days[year] = ['NaN']\n",
    "            return ['Closed during the year %d' % year]\n",
    "        for i in xrange(len(self.turn_day_year[year])):\n",
    "            if self.turn_day_year[year][i] == 0:\n",
    "                closed_days.append(self.get_date(year, i))\n",
    "        self.closed_days[year] = closed_days\n",
    "        return closed_days\n",
    "    \n",
    "    def get_low_capacity_days(self, year):\n",
    "        low_capacity_days = []\n",
    "        if year not in self.turn_day_year:\n",
    "            self.low_capacity_days[year] = ['NaN']\n",
    "            return ['Closed during the year %d' % year]\n",
    "        for i in xrange(len(self.turn_day_year[year])):\n",
    "            if self.turn_day_year[year][i] < self.turn_num_year[year] * self.__low_capacity_limit__:\n",
    "                low_capacity_days.append(self.get_date(year, i))\n",
    "        self.low_capacity_days[year] = low_capacity_days\n",
    "        return low_capacity_days\n",
    "    \n",
    "    def get_date(self, year, i):\n",
    "        return str((datetime.datetime(year,1,1)+datetime.timedelta(days=i)).date())   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_all = {}\n",
    "for x in station_name:\n",
    "    if station_name[x] not in station_all:\n",
    "        station_all[station_name[x]] = Station(station_name[x])\n",
    "def parse_turns(turns, station_all):\n",
    "    for x in turns:\n",
    "        if turns[x].station_name != 'Unknown':\n",
    "            station_all[turns[x].station_name].add_turn(turns[x])\n",
    "parse_turns(turns, station_all)\n",
    "parse_turns(turns14, station_all)\n",
    "parse_turns(turns15, station_all)\n",
    "for x in station_all:\n",
    "    for year in xrange(2013, 2016):\n",
    "        station_all[x].get_low_capacity_days(year)\n",
    "        station_all[x].get_closed_days(year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I built the dictionary of all the station objects with their station names as the keys. The station objects are instantiated in this step and one can readily look up the close_days/low_capacity_days information by supplying the station name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2013: ['2013-10-20', '2013-12-08'], 2014: [], 2015: []}\n",
      "{2013: ['2013-08-25', '2013-09-22', '2013-10-20', '2013-11-03', '2013-11-23', '2013-11-24', '2013-12-08'], 2014: ['2014-05-25'], 2015: []}\n"
     ]
    }
   ],
   "source": [
    "print station_all['CENTRAL AVE'].closed_days\n",
    "print station_all['CENTRAL AVE'].low_capacity_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of looking up the closed_days and low_capacity_days information (for 'CENTRAL AVE') from the station_all dictionary. It is found 'CENTRAL AVE' station is closed in '2013-10-20' and '2013-12-08', and is of low capacity on '2013-08-25', '2013-09-22', '2013-10-20', '2013-11-03', '2013-11-23', '2013-11-24', '2013-12-08' and '2014-05-25'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
